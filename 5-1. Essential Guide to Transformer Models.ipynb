{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5-1. Essential Guide to Transformer Models.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMzNe6u0P/mZj9aGHCphsdk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"U6rwINVpGRFG"},"source":["##### Copyright 2019 The TensorFlow Authors."]},{"cell_type":"code","metadata":{"id":"yEjgn_KHHA6k","executionInfo":{"status":"ok","timestamp":1632794134232,"user_tz":-540,"elapsed":333,"user":{"displayName":"이재훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghn1YVfD2yXCylStMwpRhLWW5v0bbynr4TSqcOQQQ=s64","userId":"14270995088772057531"}}},"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HJzKcbiDHHOn"},"source":["# Essential Guide to Transformer Models\n","\n","**참고 Link**\n","  1. [Attention is All You Need (2017)](https://arxiv.org/abs/1706.03762)\n","  2. [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n","  3. [The Illustrated Transformer(한국어 번역본)](https://nlpinkorean.github.io/illustrated-transformer/)\n","  4. [Illustrated Guide to Transformers- Step by Step Explanation](https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0)\n","  5. [Transformer: Attention Is All You Need (꼼꼼한 딥러닝 논문 리뷰와 코드 실습)](https://www.youtube.com/watch?v=AA621UofTUA&list=PLRx0vPvlEmdADpce8aoBhNnDaaHQN1Typ&index=8)\n","\n","<br>\n","\n","  - 트랜스포머가 논문에서 제안되었다. [Attention is All You Need](https://arxiv.org/abs/1706.03762).\n","  - Transformers가 자연어 처리 세계를 휩쓸고 있다.\n","    - 이 놀라운 모델들은 다수의 NLP 기록을 깨고 최신 기술을 발전시키고 있습니다.\n","    - 다음과 같은 여러 응용 프로그램에 사용됩니다.\n","        - 기계어 번역\n","        - 대화 챗봇\n","        - 구글 검색 엔진\n","\n","- NLP 밖에서도 컴퓨터 비전과 음악 발전 분야에서 트랜스포머를 찾을 수 있다.\n","- Fairwell to RNN\n","\n","  <img src='http://www.mccormickml.com/assets/BERT/BERT_Mountain.png'>\n","\n","  - [Various Model Architectures of Transformer](https://github.com/huggingface/transformers#model-architectures)\n","\n","  "]},{"cell_type":"markdown","metadata":{"id":"0dJwzWQ7Mmfb"},"source":["# Attention Mechanism"]},{"cell_type":"markdown","metadata":{"id":"RsuwpfHTMtbQ"},"source":["transformers를 이해하기 위해서는 우선 주의 메커니즘을 이해해야 한다.\n","\n","주의 메커니즘을 통해 transformers는 매우 긴 메모리를 가질 수 있습니다.<br>transformers 모델은 생성된 모든 이전 토큰에 \"참석\"하거나 \"포커스\"할 수 있습니다.\n","\n","<br>\n","<img src='https://miro.medium.com/max/1050/0*CoVkXTogdhlfjSw_.png'>\n","<br><br>\n","\n","<img src='https://miro.medium.com/max/960/0*ODlgeguKzjyzjuuJ.gif'>\n","\n","<br><br>\n"]},{"cell_type":"markdown","metadata":{"id":"pQzesS67M4s7"},"source":["## Recurrent neural networks (RNN)의 단점"]},{"cell_type":"markdown","metadata":{"id":"-QASBltpNCrj"},"source":["- 단기 메모리(참조할 더 좁은 창)\n","이야기가 길어지면, RNN은 이 순서에서 앞서 생성된 단어에 접근할 수 없다.\n","- 게이트 순환 장치(GRU) 및 LSTM(Long-Short Term Memory) 네트워크의 경우에도 마찬가지입니다.\n","- RNN은 순차적이다. (병렬 컴퓨팅 없음)"]},{"cell_type":"markdown","metadata":{"id":"LOAffRVxrGp9"},"source":["# Attention Mechanism\n","\n","- 이론적으로 충분한 계산 리소스를 제공하는 주의 메커니즘은 참조할 수 있는 무한한 창을 가지고 있으므로 텍스트를 생성하는 동안 이야기의 전체 컨텍스트를 사용할 수 있습니다.\n","\n","<br>\n","\n","<img src = 'https://miro.medium.com/max/960/1*w2OJ4jfjxmlcqu6k4BdgrA.gif'>\n","\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"56nisG0BrMEe"},"source":["# High-level View of Transformer Architecture\n","\n","- 먼저 모델을 단일 블랙박스로 살펴보겠습니다.<br> 기계 번역 어플리케이션에서, 그것은 한 언어로 된 문장을 가지고 다른 언어로 그것의 번역을 출력할 것이다.\n","\n","<img src='https://jalammar.github.io/images/t/the_transformer_3.png'>\n","\n","- A different view\n","\n","<img src='https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png'>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YSgz7obDrXhS"},"source":["- 인코딩 컴포넌트는 인코더 더미입니다. (종이는 6개를 서로 겹쳐 쌓습니다. 숫자 6에는 마법이 없습니다.)\n","\n","<img src='https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png'>\n","\n","<img src= 'https://hackernoon.com/_next/image?url=https%3A%2F%2Fcdn.hackernoon.com%2Fimages%2FZ7ZkCzqpkBdXJ4DMKYwft6zCe9q1-3t1235mt.jpeg&w=1920&q=75'>"]},{"cell_type":"markdown","metadata":{"id":"vEjGTXZKsMii"},"source":["인코더는 구조가 모두 동일하지만 가중치를 공유하지는 않습니다.<br>\n","각 층은 두 개의 하위 층으로 나뉩니다.\n","\n","<img src='https://jalammar.github.io/images/t/Transformer_encoder.png'>\n","\n","- 높은 수준에서 인코더는 입력 시퀀스를 해당 입력의 모든 학습된 정보를 저장하는 추상적인 연속 표현으로 매핑합니다.\n","\n","- 디코더에는 두 개의 층이 있지만, 그 사이에는 디코더가 입력 문장의 관련 부분에 집중할 수 있도록 도와주는 주의 계층이 있습니다.\n","\n","<img src='https://jalammar.github.io/images/t/Transformer_decoder.png'>\n","\n","- 그런 다음 높은 수준에서 디코더는 연속 표현을 취하고 단계별로 이전 출력도 공급받으면서 단일 출력을 생성합니다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"voBC1QiRs2G8"},"source":["## Detailed Look\n","\n","<img src='https://i.stack.imgur.com/eAKQu.png'>"]},{"cell_type":"markdown","metadata":{"id":"E6nlb2g0tYLV"},"source":["## Detailed View of Transformer Architecture"]},{"cell_type":"markdown","metadata":{"id":"8xKHPU6ptp6B"},"source":["```\n","대화형 챗봇에서 이 기능이 어떻게 작동하는지 보여드리겠습니다.\n","\n","입력: \"안녕하세요\"\n","트랜스포머 출력: \"괜찮습니다.\"\n","```\n","### Input Embeddings\n","첫 번째 단계는 입력 정보를 단어 임베딩 계층에 제공하는 것입니다.\n","\n","<img src= 'https://miro.medium.com/max/437/0*6MnniQMOBPu4kFq3.png'>"]},{"cell_type":"markdown","metadata":{"id":"ICSzaVFMt3qE"},"source":["### Positional Encoding\n","\n","참고 Link: \n","\n","[1. Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)\n","\n","[2. Master Positional Encoding, Part 1](https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3)\n","\n","다음 단계는 위치 정보를 임베딩에 주입하는 것입니다.\n","\n","<img src='https://d33wubrfki0l68.cloudfront.net/a76be57763d942798f8081b77edf8f078720cd45/bd31a/img/transformer_architecture_positional_encoding/model_arc.jpg'>"]},{"cell_type":"markdown","metadata":{"id":"EY5N19O2uECS"},"source":["- 단어의 위치와 순서는 모든 언어의 필수적인 부분이다.\n","    - 문법과 문장의 실제 의미를 정의한다.\n","\n","- 순환 신경 네트워크(RNN)는 본질적으로 단어 순서를 고려한다.\n","    - 순차적으로 문장을 낱말 단위로 파싱한다.\n","\n","- 그러나 트랜스포머 아키텍처는 다중 헤드 자기 주의 메커니즘을 위해 반복 메커니즘을 제거했다.\n","    - RNN의 재발 방법을 피하면 훈련 시간에 엄청난 속도가 빨라질 것이다.\n","    - 그리고 이론적으로, 그것은 문장에서 더 긴 의존성을 포착할 수 있다.\n","\n","- 문장의 각 단어가 트랜스포머의 인코더/디코더 스택을 동시에 통과하기 때문에 모델 자체에는 각 단어에 대한 위치/순서가 없습니다.\n","    - 따라서, 단어의 순서를 우리 모델에 통합하는 방법이 여전히 필요합니다.\n","\n","- 모델에 어느 정도 질서감을 주는 하나의 가능한 해결책은 문장에서의 각 단어에 그것의 위치에 대한 정보의 조각을 추가하는 것이다. 우리는 이것을 \"정보의 조각\", 위치 부호화라고 부른다. \n","    - positional encoding을 사용하여 수행됩니다. 저자들은 sin, cos 함수를 이용한 영리한 묘기를 생각해 냈다.\n","\n","<img src='https://miro.medium.com/max/1050/0*PuV87IVkFin98EZW.png'>\n","\n","<img src='https://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png'>\n","\n","<img src='https://jalammar.github.io/images/t/transformer_positional_encoding_example.png'>\n","\n","<img src='https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png'>\n"]},{"cell_type":"markdown","metadata":{"id":"QLO1xEOSu6MT"},"source":["### Encoder Layer\n","\n","<img src='https://miro.medium.com/max/846/0*gxx0-uUpZfAmKmPa.png'>\n","\n","- 인코더 레이어 작업은 모든 입력 시퀀스를 해당 시퀀스 전체에 대해 학습된 정보를 보유하는 추상 연속 표현으로 매핑하는 것입니다.\n","- 여기에는 2개의 서브모듈, 다중 헤드 어텐션, 그리고 완전히 연결된 네트워크가 포함되어 있습니다.\n","\n","<img src='https://hackernoon.com/_next/image?url=https%3A%2F%2Fcdn.hackernoon.com%2Fimages%2FZ7ZkCzqpkBdXJ4DMKYwft6zCe9q1-bx1e35i5.jpeg&w=1920&q=75'>\n","\n","- 2개의 서브레이어 주변에는 층 정규화에 따른 잔여 연결부도 있습니다.\n","- 잔여 연결부는 '스킵 연결부'와 동일합니다. 이러한 구배는 비선형 활성화 함수를 통과하지 않고 네트워크를 통해 직접 흐를 수 있도록 하기 위해 사용됩니다.\n","- 계층 정규화는 네트워크를 안정화시키기 위해 사용되므로 필요한 교육 시간이 상당히 단축됩니다.\n"]},{"cell_type":"markdown","metadata":{"id":"xbvcA33lxPfW"},"source":["### Data Flow\n","\n","- 인코더 블록은 입력이 $S\\times D$ 형태일 것으로 예상한다.\n","- $S$ = 문장 길이\n","- $D$ = 네트워크에서 가중치를 훈련할 수 있는 임베딩의 치수.\n","- 입력 및 출력 형태가 동일하다는 점에 유의하십시오.\n","\n","<img src='https://hackernoon.com/_next/image?url=https%3A%2F%2Fcdn.hackernoon.com%2Fimages%2FZ7ZkCzqpkBdXJ4DMKYwft6zCe9q1-lu1s35gs.jpeg&w=1920&q=75'>"]},{"cell_type":"markdown","metadata":{"id":"u8mg-vKPxaqr"},"source":["### Multi-Headed Attention\n","\n","- 인코더의 다중 헤드 주의는 자기 주의라고 하는 특정 주의 메커니즘을 적용한다.\n","\n","- 모델은 self 어텐션을 통해 입력에서 각 단어를 다른 단어와 연결할 수 있습니다.\n","\n","- 따라서 이 예제에서 모델은 \"you\"이라는 단어를 \"how\" 및 \"are\"와 연결하는 방법을 배울 수 있습니다.\n","- 또한 모델은 이러한 패턴으로 구성된 단어가 일반적으로 질문임을 학습하므로 적절하게 응답할 수 있습니다.\n","\n","<img src = 'https://miro.medium.com/max/669/0*mupwYy99Watj1GJT.png'>\n","\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"qydVNeGU3Xz2"},"source":["### Self-Attention Layer\n"]},{"cell_type":"markdown","metadata":{"id":"lVJ5e2Vi3lwV"},"source":["<img src= 'https://pbs.twimg.com/media/Et1JSjMVEAAGgLT?format=png&name=900x900'>\n","\n","<br>\n","<br>\n","<img src='https://hackernoon.com/_next/image?url=https%3A%2F%2Fcdn.hackernoon.com%2Fimages%2FZ7ZkCzqpkBdXJ4DMKYwft6zCe9q1-762235qa.jpeg&w=3840&q=75'>\n","\n","- self-attention layer 는 Q는 쿼리($W_Q$), 키($W_K$), 값($W_V$)_의 3가지 가중치 매트릭스로 초기화됩니다.\n","- 각 매트릭스의 크기는 $D\\times d$이며, 여기서 $d$는 \"Attention is your need\"라는 논문에서 64로 간주된다.\n","\n","- step 0: (그림의 Calc 1), 입력에 쿼리, 키, 값 행렬을 곱하여 행렬 Q, K, V를 만든다.\n","<br>\n","      $$Q=X\\cdot W_Q$$ \n","      $$K=X\\cdot W_K$$\n","      $$V=X\\cdot W_V$$\n","\n","- step 1: $Q$에 $K^T$를 곱하여 점수 행렬을 계산합니다.\n","<br>\n","$$ \\text{Attention Score} = Q\\cdot K^T, \\text{dim} = S\\times S  $$\n","\n","- 점수 매트릭스는 단어를 다른 단어에 얼마나 집중해야 하는지를 결정합니다.\n","- 그래서 각 단어는 시간 단계의 다른 단어에 해당하는 점수를 가집니다. 점수가 높을수록 집중력이 높아집니다. 이렇게 하면 쿼리가 키에 매핑됩니다.\n","\n","- step 2: \\sqrt d$를 나누어 위의 주의 점수 매트릭스를 축소합니다.\n","\n","- step 3: 스케일링된 주의 점수에 소프트맥스 기능을 적용합니다.\n","<br>\n","$$\\text{softmax} \\bigg(\\frac {Q\\cdot K^T}{\\sqrt d} \\bigg)$$\n","\n","- step 4: 소프트맥스 출력과 값 벡터 V를 곱하여 출력 벡터를 구합니다.\n","\n","$$S = D(Q,K,V) = \\text{softmax} \\bigg(\\frac {Q\\cdot K^T}{\\sqrt d} \\bigg)\\cdot V, \\text {dim} = S \\times d$$\n","\n","- 소프트맥스 점수가 높을수록 모델이 학습하는 단어의 가치가 유지됩니다.\n","- 점수가 낮으면 관련 없는 단어가 빠져 버릴 것이다.\n","그리고 나서 그 결과물을 처리할 선형 레이어에 공급합니다."]},{"cell_type":"markdown","metadata":{"id":"QTrEISlX301F"},"source":["### Query, Key, and Value Vectors\n","\n","To achieve self-attention, 우리는 3개의 완전히 연결된 계층에 입력을 공급하여 쿼리, 키 및 값 벡터를 만든다.\n","이 벡터들은 정확히 무엇입니까?\n","\n","I found a [good explanation on stack exchange](https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms) stating….\n","\n","> 쿼리 키와 값 개념은 검색 시스템에서 가져옵니다.\n",">\n","> 예를 들어, 유튜브에서 일부 동영상을 검색하기 위해 쿼리를 입력하면 검색 엔진은 데이터베이스의 후보 동영상과 연결된 키 집합(비디오 제목, 설명 등)에 대해 쿼리를 매핑한 다음 가장 적합한 동영상(값)을 제공합니다.\n",">"]},{"cell_type":"markdown","metadata":{"id":"nMZFgUik5lyT"},"source":["### Step by Step Detailed Guide\n","\n","  - Step 0: 입력에 쿼리, 키 및 값 행렬을 곱하여 행렬 Q, K, V를 계산합니다.\n","respective Query, Key, and Value matrix.\n","\n","      $$Q=X\\cdot W_Q$$ \n","      $$K=X\\cdot W_K$$\n","      $$V=X\\cdot W_V$$\n","\n","  - Step 1: $Q$ with $K^T$를 곱하여 점수 행렬을 계산한다.\n","\n","      $$ \\text{Attention Score} = Q\\cdot K^T, \\text{dim} = S\\times S  $$\n","\n","    <img src= 'https://miro.medium.com/max/960/1*kxR_DjBgFw7LTTN-Ut34Pw.gif'>\n","\n","    <img src ='https://miro.medium.com/max/675/0*D8GA7_DsjTfudI0-.png'>\n","\n","<br>\n","\n","    - 각 단어는 시간 단계의 다른 단어에 해당하는 점수를 가집니다.\n","    - 점수가 높을수록 집중력이 높아집니다. 이렇게 하면 쿼리가 키에 매핑됩니다.\n","\n","  - Step 2: 위의 attention 점수 매트릭스를 $\\sqrt d$로 나누어 축소한다.\n","\n","    <img src='https://miro.medium.com/max/716/0*VKdRa5TC6jJmKZ-0.png'>\n","\n","  - Step 3: softmax 기능을 스케일링된 attention score에 적용\n","\n","      $$\\text{softmax} \\bigg(\\frac {Q\\cdot K^T}{\\sqrt d} \\bigg)$$\n","\n","    <img src='https://miro.medium.com/max/1050/0*qSKUxncfQVhUJeCr.png'>\n","\n","  - Step 4: 소프트맥스 출력과 값 벡터 V를 곱하면 출력 벡터를 얻을 수 있습니다.\n","\n","      $$S = D(Q,K,V) = \\text{softmax} \\bigg(\\frac {Q\\cdot K^T}{\\sqrt d} \\bigg)\\cdot V, \\text {dim} = S \\times d$$\n","\n","      <img src='https://miro.medium.com/max/951/0*dv4q_I1gubq2xua1.png'>\n","\n","    - 소프트맥스 점수가 높을수록 모델이 학습하는 단어의 가치가 유지됩니다.\n","    - 점수가 낮으면 관련 없는 단어가 빠져 버릴 것이다.\n","    - 그리고 나서 그 결과물을 처리할 선형 레이어에 공급합니다.\n","  \n"]},{"cell_type":"markdown","metadata":{"id":"se68ZBLE5qmJ"},"source":["### Computing Multi-headed Attention\n","\n","Multi-head Attention은 Attention 메커니즘을 여러 번 병렬로 실행하는 Attention 메커니즘을 위한 모듈입니다. <br>\n","그런 다음 독립Attention 출력이 연결되고 예상 차원으로 선형 변환된다.<br>\n","직관적으로, multiple attention heads는 시퀀스의 일부(e.g. : 장기 의존성 대 단기 의존성)에 다르게 주의를 기울일 수 있다.\n","\n","<img src='https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png'>\n","\n","<img src ='https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png'>\n","\n","  <img src='https://jalammar.github.io/images/t/transformer_attention_heads_z.png'>\n","\n","<img src='https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png'>\n","\n","<img src='https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png'>\n","\n","<img src='https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png'>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FM2NZfDz58f1"},"source":["잔류 연결, 계층 표준화 및 피드 포워드 네트워크\n","- multi-headed attention 출력 벡터는 원래 위치 입력 임베딩에 추가된다. 이를 residual connection.이라고 합니다.\n","\n","- residual connection.의 출력은  layer normalization.를 거칩니다.\n","\n","<img src = 'https://miro.medium.com/max/960/0*RRWR2BsH5SQgMGo3.gif'>"]},{"cell_type":"markdown","metadata":{"id":"nJIfwLeO9eOG"},"source":["- normalized residual 출력은 추가 처리를 위해 점별 feed-forward 네트워크를 통해 투영됩니다.\n","- 점별 feed-forward 네트워크는 ReLU 활성화가 중간에 있는 두 개의 선형 레이어입니다.\n","- 그 다음, 그 출력은 점별 feed-forward 네트워크의 입력에 다시 추가되고 더욱 정규화됩니다."]},{"cell_type":"markdown","metadata":{"id":"tIz-fUk299V8"},"source":["<img src='https://miro.medium.com/max/960/0*-fdpoPbN-BHAMRnr.gif'>\n","\n","- residual connections는 경사가 네트워크를 통해 직접 흐를 수 있게 함으로써 네트워크 교육에 도움이 됩니다.\n","- 계층 정규화는 네트워크를 안정화시키기 위해 사용되므로 필요한 교육 시간이 상당히 단축됩니다.\n","- 점별 feed-forward layer은 잠재적으로 더 풍부한 표현을 제공하는 attention 출력을 투영하는 데 사용됩니다."]},{"cell_type":"markdown","metadata":{"id":"-xFa2ydQ-QRs"},"source":["### Decoder Layer"]},{"cell_type":"markdown","metadata":{"id":"_ZbCWucOL5bD"},"source":["- 디코더의 역할은 텍스트 시퀀스를 생성하는 것입니다.\n","- 디코더는 인코더와 유사한 하위 계층을 가지고 있습니다.\n","    - 두 개의  multi-headed attention layers, pointwise feed-forward layer, residual connections 및 각 하위 계층 이후의 layer normalization를 가지고 있습니다.\n","    - 이 서브 레이어는 인코더의 레이어와 비슷하게 동작하지만 각 multi-headed attention layer는 다른 역할을 합니다.\n","- 디코더는 분류기 역할을 하는 선형 레이어와 단어 확률을 얻기 위한 소프트맥스로 차단됩니다.\n"," <img src = 'https://miro.medium.com/max/3600/0*bXI7F0OiEtbtyy3k.png'>\n","\n"," - 디코더는 autoregressive이며, 시작 토큰으로 시작하며, 입력으로부터 attention 정보를 포함하는 인코더 출력뿐만 아니라 이전 출력의 목록을 입력으로 받아들입니다. 토큰을 출력으로 생성할 때 디코더는 디코딩을 중지합니다.\n","\n"," <img src = 'https://miro.medium.com/max/720/0*u8nSpT8Z8ITwzNLV.gif'>"]},{"cell_type":"markdown","metadata":{"id":"wYwnvhJ9MbkA"},"source":["### Decoder Input Embeddings & Positional Encoding\n","\n","- 디코더의 시작은 인코더와 거의 같습니다.\n","- 입력은 임베딩 계층과 위치 인코딩 계층을 거칩니다.\n","positional embeddings은 디코더 입력에 대한 attention scores를 계산하는 첫 번째 multi-head attention layer에 공급된다."]},{"cell_type":"markdown","metadata":{"id":"PWeQdSB1NlHT"},"source":["### Decoders First Multi-Headed Attention\n","\n","- 이 multi-headed attention layer는 약간 다르게 작동합니다.\n","- 디코더는 autoregressive이며 단어별로 시퀀스 워드를 생성하므로 미래의 토큰에 대한 조건화를 방지해야 합니다.\n","- 예) \"am\"의 주의 점수를 계산할 때, \"fine\"이라는 단어는 이후에 생성된 미래 단어이기 때문에 \"fine\"이라는 단어에 접근해서는 안 됩니다.\n","- \"am\"이라는 단어는 자신과 그 앞의 단어에만 접근할 수 있어야 합니다. 이것은 그들이 이전 단어에만 주의를 기울일 수 있는 다른 모든 단어에 적용된다.\n","\n","<img src = 'https://miro.medium.com/max/818/0*0pqSkWgSPZYr_Sjx.png'>\n","\n","- 미래 단어에 대한 attention scores를 계산하는 방법이 필요합니다.\n","- 이 방법을 __masking_이라고 합니다.\n","- 디코더가 미래 토큰을 보지 못하게 하려면 __앞을 내다보는 mask__를 적용_\n","- softmax 계산 전, 점수 스케일링 후 mask가 __추가됩니다."]},{"cell_type":"markdown","metadata":{"id":"QdgzUja4OJwl"},"source":["### Look-Ahead Mask\n","\n","- mask는 0과 음의 부정의 값으로 채워진 attention scores와 같은 크기의 매트릭스입니다.\n","- 여러분이 확대된 attention scores에 mask를 더하면, 여러분은 오른쪽 위의 삼각형이  부정사로 채워진 점수의 행렬을 얻게 됩니다.\n","\n","<img src ='https://miro.medium.com/max/1050/0*QYFua-iIKp5jZLNT.png'>\n","\n","- 마스크의 이유는 일단 마스크된 점수의 소프트맥스를 취하면 음의 부정사가 0이 되고 이후 토큰에 대한 attention scores가 0이 되기 때문이다.\n","- 아래 그림에서 볼 수 있듯이, \"am\"에 대한 attention scores는 자신과 그 앞에 있는 모든 단어에 대한 값을 가지지만 \"fine\"이라는 단어에 대해서는 0입니다\n","- 이것은 본질적으로 모델이 이러한 단어에 초점을 두지 말라는 것을 말해준다.\n","\n","<img src = 'https://miro.medium.com/max/1050/0*3ykVCJ9okbgB0uUR.png'>\n","\n","- 이 마스킹은 첫 번째 multi-headed attention layer에서 attention scores를 계산하는 방법의 유일한 차이입니다.\n","- 이 레이어는 추가 처리를 위해 선형 레이어를 통해 연결되고 공급되기 전에 마스크가 적용되고 있는 여러 헤드를 가지고 있습니다.\n","- 첫 번째 multi-headed attention layer의 출력은 모델이 디코더 입력에 어떻게 참여해야 하는지에 대한 정보를 포함하는 마스킹된 출력 벡터입니다.\n","\n","<img src='https://miro.medium.com/max/1050/0*4Qos9ymoz4LW1pmP.png'>"]},{"cell_type":"code","metadata":{"id":"IZwyGEejOsxr"},"source":[""],"execution_count":null,"outputs":[]}]}